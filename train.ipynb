{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3be18e5-6ccf-4142-b23d-4183f36bca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "from conversight import TaskLibrary,FlowLibrary,Parameter,Flow\n",
    "tsk  = TaskLibrary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8630a85b-2341-48a1-860e-dbcde980df12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flows loaded !!\n"
     ]
    }
   ],
   "source": [
    "flw= FlowLibrary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9df3ecd0-636c-442f-b7f8-332ae1ce6953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At this moment U have the following versions: ['0.1', '0.2', '0.3', '0.4', '0.5']\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flw.Training.TrainETE.getVersions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74244d83-df11-43a3-aea6-4df00721d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n",
      "\u001b[0;34m[2024-01-03 16:00:02,846] [INFO] Processing has been initiated sme upload\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:00:02,847] [INFO] sme processing has been initiated for getting sme qdrant records\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006242036819458008,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading .gitattributes",
       "rate": null,
       "total": 1173,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5ee5786baf41bba50de8f5338a9490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading .gitattributes:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005887269973754883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading 1_Pooling/config.json",
       "rate": null,
       "total": 190,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361ac897b6524e26b0a265b31b0b91b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006086826324462891,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading README.md",
       "rate": null,
       "total": 10135,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0199df0d46ac4a1498c98c5d7199704a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading README.md:   0%|          | 0.00/10.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005433320999145508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 612,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c8e77cb2d342388f0dc876ebefd83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005478620529174805,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)ce_transformers.json",
       "rate": null,
       "total": 116,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "217f3e9475c6417ebbfc81e313d6878e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006606101989746094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data_config.json",
       "rate": null,
       "total": 39265,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87eaa9c76efc4978baec8d9d303744cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00658726692199707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 90888945,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245ebc09d45b43e38fbf9704a7eec857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005391597747802734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)nce_bert_config.json",
       "rate": null,
       "total": 53,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719228c47f494d9b862d320bd84418b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009694337844848633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading (…)cial_tokens_map.json",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634feb4bf8b64d94940a0f5f65d5aa9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0058155059814453125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer.json",
       "rate": null,
       "total": 466247,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa17f7fadcb412d93f0595b63a9d633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005846977233886719,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading tokenizer_config.json",
       "rate": null,
       "total": 350,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a622566ab2e742c68197176a1965bd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006496906280517578,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading train_script.py",
       "rate": null,
       "total": 13156,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8652c8f8b7c74388bc5b45a9a7953cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005347013473510742,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading vocab.txt",
       "rate": null,
       "total": 231508,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28781c7d9104431784ab88815a5179d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005422353744506836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading modules.json",
       "rate": null,
       "total": 349,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9070f36a1a42909505da0f57003832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m[2024-01-03 16:00:59,478] [INFO] sme processing has been completed and records formed\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:00:59,752] [INFO] Creating or overwriting collection 6465f504-zXTxrQU4i-sme-v2...\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:01:00,099] [INFO] Collection 6465f504-zXTxrQU4i-sme-v2 overwritten successfully.\u001b[0m\n",
      "Tasks loaded  !!\n",
      "\u001b[0;34m[2024-01-03 16:01:00,514] [INFO] Qdrant upload function initiated and imports made successfully\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:01:00,858] [INFO] collection actions function has been initiated and imports completed\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:01:00,866] [INFO] client assigned\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[0;34m[2024-01-03 16:01:05,195] [INFO] [2024-01-03 16:01:05] Upload completed successfully. Time taken: 0:00:04.680158\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:01:05,196] [INFO] sme upload has been completed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "coll = datasetId + '-sme-v2'\n",
    "resp = tsk.Training.smeEntityUpload.run(datasetId, type=['sme'], smeCollectionName=coll, entityCollectionName=None, hostURL=\"http://qdrant-new.qdrant-new.svc.cluster.local:6333\", apiKey='Y52nuGtA0T0Eekx6xryy9AWDZwmIbaWt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72828c98-f0d9-4f7e-a5f0-465797064911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n",
      "\u001b[0;34m[2024-01-03 16:22:03,759] [INFO] Loading dataset Market_Analytics...\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:04,138] [INFO] ***** 6465f504-zXTxrQU4i Dataset object created.*****\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:04,348] [INFO] Dataset ID -> 6465f504-zXTxrQU4i\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:04,444] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Campaign/part-00000-0ff5ed2b-8a2b-44c1-bd33-bc996b9d6edc-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:04,457] [INFO] Embedding the content of entity data. Total data to embed -> 16\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:04,551] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Channel_Campaign_Name/part-00000-f5cd60e2-bf3c-4b46-b93d-da50615d604a-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:04,567] [INFO] Embedding the content of entity data. Total data to embed -> 116\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,296] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Program/part-00000-de5df77e-4954-4606-aa86-1e19dfe2fe49-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,310] [INFO] Embedding the content of entity data. Total data to embed -> 12\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,358] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Creative_type/part-00000-5e6ec72e-39a4-4e22-82b9-4dc771c609fd-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,373] [INFO] Embedding the content of entity data. Total data to embed -> 3\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,400] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Target_Audience/part-00000-3da2667d-25cd-4c5b-af70-1e2d4bba913b-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,413] [INFO] Embedding the content of entity data. Total data to embed -> 4\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,448] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Pillar/part-00000-c5afbec4-7257-42fb-b530-d100fa7c5140-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,463] [INFO] Embedding the content of entity data. Total data to embed -> 4\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,494] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Channel/part-00000-b6f5df38-c067-44a3-b2e9-8f32ab8efc07-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,507] [INFO] Embedding the content of entity data. Total data to embed -> 19\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,607] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Platform/part-00000-2f6690d8-6f5e-4b92-997d-94844f99ddd1-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,619] [INFO] Embedding the content of entity data. Total data to embed -> 5\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,652] [INFO] Processing file -> /home/jovyan/data/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/m_659510ca/dictionary/gs_pivot_data/Objective/part-00000-89acced1-6a1b-4c93-bf11-2a6bd25eac78-c000.csv\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,664] [INFO] Embedding the content of entity data. Total data to embed -> 3\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,687] [INFO] Concatenating all the processed data\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,726] [INFO] Successfully processed entity data\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:05,728] [INFO] Embedding the content of entity data. Total data to embed -> 182\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:07,020] [INFO] Creating or overwriting collection 6465f504-zXTxrQU4i-entity...\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:07,366] [INFO] Collection 6465f504-zXTxrQU4i-entity created successfully.\u001b[0m\n",
      "Tasks loaded  !!\n",
      "\u001b[0;34m[2024-01-03 16:22:07,741] [INFO] Qdrant upload function initiated and imports made successfully\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:08,102] [INFO] collection actions function has been initiated and imports completed\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:08,111] [INFO] client assigned\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:22:10,373] [INFO] [2024-01-03 16:22:10] Upload completed successfully. Time taken: 0:00:02.631387\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "coll = datasetId + '-entity'\n",
    "resp = tsk.Training.smeEntityUpload.run(datasetId, type=['entity'], smeCollectionName=None, entityCollectionName=coll, hostURL=\"http://qdrant-new.qdrant-new.svc.cluster.local:6333\", apiKey='Y52nuGtA0T0Eekx6xryy9AWDZwmIbaWt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82bb54c2-cb91-4a24-a558-7115120f56dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': 'Upload completed successfully for the given records'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a951585-538f-4920-a346-9cd5f6c9086a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b6e07d40-bcb6-41b4-bdc9-ead1e3d1f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant_data_retriver(\"http://qdrant-new.qdrant-new.svc.cluster.local:6333/collections/{}/points/scroll\".format('6465f504-zXTxrQU4i-entity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1abc64d8-c059-4c6d-9ed1-e25827e335f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @task\n",
    "def qdrant_data_retriver(qdrant_api_url):\n",
    "    \"\"\"\n",
    "        Retrieve data from a Qdrant API endpoint.\n",
    "    \n",
    "        This function sends a GET request to a Qdrant API endpoint specified by the 'api_url'\n",
    "        parameter and retrieves data from it. Qdrant is a vector search engine that stores\n",
    "        and allows efficient retrieval of high-dimensional vectors.\n",
    "    \n",
    "        Parameters:\n",
    "        - api_url (str): The URL of the Qdrant API endpoint to retrieve data from.\n",
    "    \n",
    "        Returns:\n",
    "        - dict: A dictionary containing the response data from the API. The structure of\n",
    "          this dictionary may vary depending on the specific Qdrant API endpoint and\n",
    "          the data it provides. Common fields in the response may include 'status_code',\n",
    "          'message', and 'data'. The 'data' field typically contains the retrieved data.\n",
    "    \n",
    "        Raises:\n",
    "        - requests.exceptions.RequestException: If there is an issue with the HTTP request\n",
    "          (e.g., connection error, timeout, or invalid URL).\n",
    "        - ValueError: If the 'api_url' parameter is empty or invalid.\n",
    "    \n",
    "        Example:\n",
    "        ```\n",
    "        api_url = \"https://example.com/qdrant/api/v1/retrieve\"\n",
    "        response = qdrant_data_retriever(api_url)\n",
    "        if response['status_code'] == 200:\n",
    "            data = response['data']\n",
    "            # Process and use the retrieved data as needed.\n",
    "        else:\n",
    "            print(f\"Error: {response['message']}\")\n",
    "        ```\n",
    "    \n",
    "        Note:\n",
    "        - Make sure to replace 'api_url' with the actual URL of the Qdrant API endpoint you\n",
    "          want to retrieve data from.\n",
    "        - You may need to install the 'requests' library if it's not already installed to\n",
    "          use this function. You can install it with `pip install requests`.\n",
    "        - Always handle exceptions and errors appropriately when using this function to\n",
    "          ensure graceful error handling and prevent application crashes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        def Merge(dict1, dict2):\n",
    "            res = {**dict1, **dict2}\n",
    "            return res\n",
    "        \n",
    "    \n",
    "        import pandas as pd\n",
    "        import requests\n",
    "        from conversight import Context\n",
    "        \n",
    "        ctx = Context()\n",
    "        next_page_offset = ''\n",
    "        df = pd.DataFrame()\n",
    "        \n",
    "        ctx.log.info(\"Started Fetching\")\n",
    "        while next_page_offset is not None:\n",
    "            if next_page_offset == \"\":\n",
    "                request_payload = {\n",
    "                \"with_payload\": True,\n",
    "                \"with_vector\": True,\n",
    "                \"match\": {},\n",
    "               \"limit\": 100\n",
    "               #\"offset\": next_page_offset\n",
    "            }\n",
    "            else:\n",
    "                request_payload = {\n",
    "                \"with_payload\": True,\n",
    "                \"with_vector\": True,\n",
    "                \"match\": {},\n",
    "               \"limit\": 100,\n",
    "               \"offset\": next_page_offset\n",
    "            }\n",
    "        \n",
    "            response = requests.post(qdrant_api_url, json=request_payload,headers = {\n",
    "        'API-Key': 'Y52nuGtA0T0Eekx6xryy9AWDZwmIbaWt'\n",
    "    })\n",
    "        \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                # return data\n",
    "                \n",
    "                #df = pd.DataFrame()\n",
    "                for i in range(len(data['result']['points'])):\n",
    "                    d = {}\n",
    "                    d['id'] = data['result']['points'][i]['id']\n",
    "                    d['vector'] = str(data['result']['points'][i]['vector'])\n",
    "                    for j in data['result']['points'][i]['payload'].keys():\n",
    "                        if type(data['result']['points'][i]['payload'][j]) == str:\n",
    "                            d[j] = data['result']['points'][i]['payload'][j]\n",
    "                    d2 =  data['result']['points'][i]['payload']['metadata']\n",
    "                    d = Merge(d, d2)\n",
    "                    df2 = pd.DataFrame(d.values()).T\n",
    "                    df2.columns = d.keys()\n",
    "                    df = pd.concat([df,df2])\n",
    "                next_page_offset = data['result']['next_page_offset']\n",
    "                print('Data fetch from end point -> ',next_page_offset)\n",
    "                        \n",
    "            else:\n",
    "                #ctx.log.info('API ERROR')\n",
    "                #return response.status_code\n",
    "                pass\n",
    "        df = df.reset_index().drop(['index'],axis = 1)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df8a3906-cebd-40ed-8170-57f2db0d1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['hostURL'] = 'http://qdrant-new.qdrant-new.svc.cluster.local:6333'\n",
    "# os.environ['apiKey'] = 'Y52nuGtA0T0Eekx6xryy9AWDZwmIbaWt'\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = 'AKIAY5TYDKKLKFYUDYMI'\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = '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'\n",
    "qDrantConfiguration = {'hostURL':'http://qdrant-new.qdrant-new.svc.cluster.local:6333','apiKey':'Y52nuGtA0T0Eekx6xryy9AWDZwmIbaWt'}\n",
    "awsConfiguration = {'AWS_ACCESS_KEY_ID':'AKIAY5TYDKKLKFYUDYMI','AWS_SECRET_ACCESS_KEY':'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'}\n",
    "datasetId='6465f504-zXTxrQU4i'\n",
    "combinations=2\n",
    "output='upload'\n",
    "# operation=[\"synthetic\"]\n",
    "operation=[\"paraphrase\"]\n",
    "vectorOperation=True\n",
    "collectionAction = 'create'\n",
    "bucketName='stag-research'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06395ec4-2923-4b55-a2a0-3b2ebb54d365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks loaded  !!\n"
     ]
    }
   ],
   "source": [
    "tsk.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fe65d2f-d2b8-40c4-9031-cf28860a822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsk.Training.trainETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cdb36b5-dd83-448e-a6d1-96727e4ae43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;34m[2024-01-03 16:19:35,728] [INFO] 2024-01-03 16:19:35.728694 - syntheticParaGeneration was initiated\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:35,851] [INFO] Loading dataset Market_Analytics...\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:36,249] [INFO] ***** 6465f504-zXTxrQU4i Dataset object created.*****\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:36,250] [INFO] 2024-01-03 16:19:36.250383-- paraphrasing operations have been initiated\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:37,072] [INFO] Fetching ==>> synthentic-utterance/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/2024/1/3/6465f504-zXTxrQU4i_03-01-2024_16:18:21.parquet || from S3\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:37,073] [INFO] ----------------fetching data ---------------------\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:37,999] [INFO] *****************************************************************\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:38,000] [INFO] ----------------initiating paraphrasing---------------------\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:38,000] [INFO] *****************************************************************\u001b[0m\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[0;34m[2024-01-03 16:19:39,430] [INFO] Batch of [6] is completed and time taken is -> 1.420109748840332 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:39,609] [INFO] Batch of [16] is completed and time taken is -> 1.599592924118042 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:39,763] [INFO] Batch of [22] is completed and time taken is -> 1.7529020309448242 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:39,803] [INFO] Batch of [31] is completed and time taken is -> 1.7933673858642578 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:40,066] [INFO] Batch of [41] is completed and time taken is -> 2.0562679767608643 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:40,121] [INFO] Batch of [51] is completed and time taken is -> 2.1115798950195312 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:40,316] [INFO] Batch of [66] is completed and time taken is -> 2.3063907623291016 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:40,632] [INFO] Batch of [81] is completed and time taken is -> 2.6220531463623047 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:41,069] [INFO] Batch of [91] is completed and time taken is -> 3.0587213039398193 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:41,250] [INFO] Batch of [97] is completed and time taken is -> 3.2398130893707275 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:41,758] [INFO] Batch of [114] is completed and time taken is -> 3.7479610443115234 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:41,896] [INFO] Batch of [125] is completed and time taken is -> 3.8865222930908203 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:42,452] [INFO] Batch of [140] is completed and time taken is -> 4.442603588104248 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:43,128] [INFO] Batch of [155] is completed and time taken is -> 5.118150234222412 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:43,269] [INFO] Batch of [175] is completed and time taken is -> 5.258878231048584 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:43,297] [INFO] Batch of [183] is completed and time taken is -> 5.28678035736084 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:43,369] [INFO] Batch of [198] is completed and time taken is -> 5.359520196914673 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:43,556] [INFO] Batch of [209] is completed and time taken is -> 5.546017646789551 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:44,206] [INFO] Batch of [223] is completed and time taken is -> 6.195939302444458 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:44,359] [INFO] Batch of [233] is completed and time taken is -> 6.349250555038452 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:44,448] [INFO] Batch of [248] is completed and time taken is -> 6.438034296035767 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:45,081] [INFO] Batch of [259] is completed and time taken is -> 7.071082830429077 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:45,550] [INFO] Batch of [273] is completed and time taken is -> 7.540451526641846 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:45,665] [INFO] Batch of [293] is completed and time taken is -> 7.654930830001831 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:45,717] [INFO] Batch of [304] is completed and time taken is -> 7.707239151000977 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:45,742] [INFO] Batch of [319] is completed and time taken is -> 7.732351064682007 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:46,079] [INFO] Batch of [334] is completed and time taken is -> 8.068704843521118 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:46,972] [INFO] Batch of [339] is completed and time taken is -> 8.962110757827759 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:47,010] [INFO] Batch of [349] is completed and time taken is -> 9.000086307525635 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:47,093] [INFO] Batch of [354] is completed and time taken is -> 9.083580493927002 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:47,213] [INFO] Batch of [364] is completed and time taken is -> 9.203422546386719 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:47,318] [INFO] Batch of [379] is completed and time taken is -> 9.307675123214722 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:47,398] [INFO] Batch of [394] is completed and time taken is -> 9.388433694839478 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:47,927] [INFO] Batch of [405] is completed and time taken is -> 9.917227745056152 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:48,073] [INFO] Batch of [410] is completed and time taken is -> 10.062821626663208 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:48,156] [INFO] Batch of [416] is completed and time taken is -> 10.146534442901611 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:48,543] [INFO] Batch of [426] is completed and time taken is -> 10.533549785614014 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:48,571] [INFO] Batch of [436] is completed and time taken is -> 10.561492443084717 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:48,767] [INFO] Batch of [449] is completed and time taken is -> 10.756692171096802 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:49,593] [INFO] Batch of [459] is completed and time taken is -> 11.582846641540527 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:49,625] [INFO] Batch of [464] is completed and time taken is -> 11.61475920677185 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:49,642] [INFO] Batch of [474] is completed and time taken is -> 11.631895542144775 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:49,714] [INFO] Batch of [479] is completed and time taken is -> 11.703758001327515 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:50,175] [INFO] Batch of [489] is completed and time taken is -> 12.164808988571167 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:51,163] [INFO] Batch of [504] is completed and time taken is -> 13.153318166732788 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:51,164] [INFO] No of paraphrasing for [242] utterances generated is [504] and time taken is -> 13.153940677642822 seconds.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:51,166] [INFO] \u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:51,166] [INFO] ---------------------------------\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:51,167] [INFO] DOING VECTOR PROCESSING\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:51,167] [INFO] ---------------------------------\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:54,683] [INFO] -----------completed----------------------\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:55,908] [INFO] ---------------------------------\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:55,909] [INFO] DATA PUSHED TO S3\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:55,909] [INFO] ---------------------------------\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:55,924] [INFO] 2024-01-03 16:19:55.924566-- paraphrasing operations have been completed\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:55,925] [INFO] 2024-01-03 16:19:55.925871-- syntheticParaGeneration was completed and entered for QDrant upload\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:55,971] [INFO]  convertParquetToPandas function has been initiated successfully-->2024-01-03 16:19:55\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:57,018] [INFO] Converted Parquet data to Pandas DataFrame from: synthentic-utterance/15cc5821-a40c-4c30-b308-b24a6dc02047/6465f504-zXTxrQU4i/2024/1/3/6465f504-zXTxrQU4i_03-01-2024_16:19:54.parquet, Modified Date: 2024-01-03 16:19:56+00:00\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:57,021] [INFO] Read the latest file respective to the orgId and datasetId given\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:57,045] [INFO] Imports completed and Create records function was initiated ...\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:58,556] [INFO] [2024-01-03 16:19:58] Records for VDB upload were created. Time taken: 0:00:01.511088\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:58,557] [INFO] Creating or overwriting collection 6465f504-zXTxrQU4i-train...\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:58,853] [INFO] Collection 6465f504-zXTxrQU4i-train overwritten successfully.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:58,854] [INFO] Qdrant upload function initiated and imports made successfully\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:58,855] [INFO] collection actions function has been initiated and imports completed\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:58,855] [INFO] Creating or overwriting collection 6465f504-zXTxrQU4i-train...\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:59,149] [INFO] Collection 6465f504-zXTxrQU4i-train overwritten successfully.\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:59,150] [INFO] [2024-01-03 16:19:59] Collection action create completed. Time taken: 0:00:00.294353\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:19:59,157] [INFO] client assigned\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:20:02,755] [INFO] [2024-01-03 16:20:02] Upload completed successfully. Time taken: 0:00:03.899935\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:20:02,756] [INFO] Successfully uploaded data to qDrant for collection: 6465f504-zXTxrQU4i-train\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:20:02,764] [INFO] 2024-01-03 16:20:02.764169-- QDrant upload completed \u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:20:02,817] [INFO] GPT models are already there, configurations updated!\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:20:02,868] [INFO] Update Result: Updated successfully\u001b[0m\n",
      "\u001b[0;34m[2024-01-03 16:20:02,868] [INFO] End to end training has been completed and model has been initiated for given dataset-->6465f504-zXTxrQU4i\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "resp = tsk.Training.trainETE.run(datasetId,combinations,output,operation,vectorOperation,awsConfiguration,qDrantConfiguration,collectionAction,bucketName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5278303a-3372-424d-8f31-b6373694b9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success',\n",
       " 'message': 'Upload completed successfully for the given records'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a852f7-a492-4f7a-9766-7d0c4288c9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "648f9436-4faa-40f7-bc67-02e52584f22e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Flow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mFlow\u001b[49m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainETE\u001b[39m\u001b[38;5;124m\"\u001b[39m, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis flow will generates synthetic/paraphrase data and do upload into s3 on demand as well as enables model\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m flow:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     qDrantConfiguration \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhostURL\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://qdrant-new.qdrant-new.svc.cluster.local:6333\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiKey\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY52nuGtA0T0Eekx6xryy9AWDZwmIbaWt\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Flow' is not defined"
     ]
    }
   ],
   "source": [
    "with Flow(name=\"trainETE\", description=\"this flow will generates synthetic/paraphrase data and do upload into s3 on demand as well as enables model\") as flow:\n",
    "    import os\n",
    "    qDrantConfiguration = {'hostURL':'http://qdrant-new.qdrant-new.svc.cluster.local:6333','apiKey':'Y52nuGtA0T0Eekx6xryy9AWDZwmIbaWt'}\n",
    "    awsConfiguration = {'AWS_ACCESS_KEY_ID':'AKIAY5TYDKKLKFYUDYMI','AWS_SECRET_ACCESS_KEY':'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'}\n",
    "    datasetId=Parameter(\"Dataset ID\",'6465f504-zXTxrQU4i')\n",
    "    combinations=Parameter(\"combinations\",2)\n",
    "    output=Parameter(\"output\",'upload')\n",
    "    operation=Parameter(\"operation\",[\"synthetic\",\"paraphrase\"])\n",
    "    vectorOperation=Parameter(\"vectorOperation\",True) \n",
    "    collectionAction = 'create'\n",
    "    bucketName=Parameter(\"bucketName\",'stag-research')\n",
    "    resp = tsk.Training.trainETE(datasetId,combinations,output,operation,vectorOperation,awsConfiguration,qDrantConfiguration,collectionAction,bucketName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "769485f7-86b5-4b8e-afa8-1b257e5ba95d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run() takes 1 positional argument but 10 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m resp1 \u001b[38;5;241m=\u001b[39m \u001b[43mflw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrainETE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasetId\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombinations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorOperation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mawsConfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqDrantConfiguration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollectionAction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbucketName\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: run() takes 1 positional argument but 10 were given"
     ]
    }
   ],
   "source": [
    "resp1 = flw.Training.TrainETE.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bef1f7-31a8-40cd-88db-ff691ffbba13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
